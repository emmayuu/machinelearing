{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Example #1\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Prerequisites and Preprocessing](#Prequisites-and-Preprocessing)\n",
    "  1. [Permissions and environment variables](#Permissions-and-environment-variables)\n",
    "  2. [Data ingestion](#Data-ingestion)\n",
    "  3. [Data inspection](#Data-inspection)\n",
    "  4. [Data conversion](#Data-conversion)\n",
    "3. [Training the Tensorflow model](#Training-the-Tensorflow-model)\n",
    "4. [Set up hosting for the model](#Set-up-hosting-for-the-model)\n",
    "5. [Validate the model for use](#Validate-the-model-for-use)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to our first end-to-end example! Today, we're working through a classification problem, specifically of images of handwritten digits, from zero to nine. Let's imagine that this dataset doesn't have labels, so we don't know for sure what the true answer is. In later examples, we'll show the value of \"ground truth\", as it's commonly known.\n",
    "\n",
    "Today, however, we need to get these digits classified without ground truth. A common method for doing this is a set of methods known as \"clustering\", and in particular, the method that we'll look at today is called Tensorflow clustering.  A TensorFlow \"cluster\" is a set of \"tasks\" that participate in the distributed execution of a TensorFlow graph. Each task is associated with a TensorFlow \"server\", which contains a \"master\" that can be used to create sessions, and a \"worker\" that executes operations in the graph.\n",
    "\n",
    "To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prequisites and Preprocessing\n",
    "\n",
    "### Permissions and environment variables\n",
    "\n",
    "Here we set up the linkage and authentication to AWS services. There are two parts to this:\n",
    "\n",
    "1. The role(s) used to give learning and hosting access to your data. Here we extract the role you created earlier for accessing your notebook.  See the documentation if you want to specify  a different role\n",
    "1. The S3 bucket name and locations that you want to use for training and model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket='<emmayu>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion\n",
    "\n",
    "Next, we read the dataset from the existing repository into memory, for preprocessing prior to training.  In this case we'll use the MNIST dataset, which contains 70K 28 x 28 pixel images of handwritten digits.  For more details, please see [here](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "This processing could be done *in situ* by Amazon Athena, Apache Spark in Amazon EMR, Amazon Redshift, etc., assuming the dataset is present in the appropriate location. Then, the next step would be to transfer the data to S3 for use in training. For small datasets, such as this one, reading into memory isn't onerous, though it would be for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 948 ms, sys: 244 ms, total: 1.19 s\n",
      "Wall time: 9.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pickle, gzip, numpy, urllib.request, json\n",
    "\n",
    "# Load the dataset\n",
    "urllib.request.urlretrieve(\"http://deeplearning.net/data/mnist/mnist.pkl.gz\", \"mnist.pkl.gz\")\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data inspection\n",
    "\n",
    "Once the dataset is imported, it's typical as part of the machine learning process to inspect the data, understand the distributions, and determine what type(s) of preprocessing might be needed. You can perform those tasks right here in the notebook. As an example, let's go ahead and look at one of the digits that is part of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJUAAACfCAYAAADwOZspAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nihgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe696d8cba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (2,10)\n",
    "\n",
    "\n",
    "def show_digit(img, caption='', subplot=None):\n",
    "    if subplot==None:\n",
    "        _,(subplot)=plt.subplots(1,1)\n",
    "    imgr=img.reshape((28,28))\n",
    "    subplot.axis('off')\n",
    "    subplot.imshow(imgr, cmap='gray')\n",
    "    plt.title(caption)\n",
    "\n",
    "show_digit(train_set[0][30], 'This is a {}'.format(train_set[1][30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Tensorflow\n",
    "\n",
    "Once we have the data preprocessed and available in the correct format for training, the next step is to actually train the model using the data. Since this data is relatively small, it isn't meant to show off the performance of the k-means training algorithm.  \n",
    "After setting training parameters, we kick off training, and poll for status until training is completed, which in this example, takes between 7 and 11 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (45.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 45.9MB 26kB/s  eta 0:00:01    37% |████████████                    | 17.2MB 34.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading astor-0.6.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Downloading grpcio-1.10.0-cp36-cp36m-manylinux1_x86_64.whl (7.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.5MB 180kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.1.6 (from tensorflow)\n",
      "  Downloading absl-py-0.1.11.tar.gz (80kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 10.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-1.1.0.tar.gz\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow)\n",
      "Collecting tensorboard<1.7.0,>=1.6.0 (from tensorflow)\n",
      "  Downloading tensorboard-1.6.0-py3-none-any.whl (3.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.1MB 452kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow)\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading gast-0.2.0.tar.gz\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow)\n",
      "Collecting html5lib==0.9999999 (from tensorboard<1.7.0,>=1.6.0->tensorflow)\n",
      "  Downloading html5lib-0.9999999.tar.gz (889kB)\n",
      "\u001b[K    100% |████████████████████████████████| 890kB 1.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8 (from tensorboard<1.7.0,>=1.6.0->tensorflow)\n",
      "  Downloading Markdown-2.6.11-py2.py3-none-any.whl (78kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 11.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting bleach==1.5.0 (from tensorboard<1.7.0,>=1.6.0->tensorflow)\n",
      "  Downloading bleach-1.5.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow)\n",
      "Building wheels for collected packages: absl-py, termcolor, gast, html5lib\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/3c/0f/0a/6c94612a8c26070755559045612ca3645fea91c11f2148363e\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/de/f7/bf/1bcac7bf30549e6a4957382e2ecab04c88e513117207067b03\n",
      "  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/8e/fa/d6/77dd17d18ea23fd7b860e02623d27c1be451521af40dd4a13e\n",
      "  Running setup.py bdist_wheel for html5lib ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/6f/85/6c/56b8e1292c6214c4eb73b9dda50f53e8e977bf65989373c962\n",
      "Successfully built absl-py termcolor gast html5lib\n",
      "Installing collected packages: astor, grpcio, absl-py, termcolor, html5lib, markdown, bleach, tensorboard, gast, tensorflow\n",
      "  Found existing installation: html5lib 0.999999999\n",
      "    Uninstalling html5lib-0.999999999:\n",
      "      Successfully uninstalled html5lib-0.999999999\n",
      "  Found existing installation: bleach 2.0.0\n",
      "    Uninstalling bleach-2.0.0:\n",
      "      Successfully uninstalled bleach-2.0.0\n",
      "Successfully installed absl-py-0.1.11 astor-0.6.2 bleach-1.5.0 gast-0.2.0 grpcio-1.10.0 html5lib-0.9999999 markdown-2.6.11 tensorboard-1.6.0 tensorflow-1.6.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time              \n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define graph input \n",
    "# mnist image shape 28*28=784\n",
    "# 0-9 digits have 10 classes\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "#define output label\n",
    "y_label = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# define weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# define model -Softmax function\n",
    "\n",
    "y_predict = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimize error using cross entropy\n",
    "cross_entropy  = tf.reduce_mean(-tf.reduce_sum( y_label * tf.log(y_predict), reduction_indices=1))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession() \n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9184\n",
      "Training and Modeling time：48.699sec\n"
     ]
    }
   ],
   "source": [
    "# train 1000 times\n",
    "# Test model accuracy\n",
    "# total time used in training and prediction\n",
    "\n",
    "for _ in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(optimizer, feed_dict={x: batch_xs, y_label: batch_ys})\n",
    "correct_predict = tf.equal(tf.argmax(y_predict,1), tf.argmax(y_label, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predict, dtype='float'))\n",
    "print('Accuracy:',sess.run(accuracy, feed_dict={x: mnist.test.images, y_label: mnist.test.labels}))\n",
    "print('Training and Modeling time：%.3f' %(time() - start_time)+'sec')\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
